# ollama_flask_langchain_web
Using ollama nlp LLM to combine with flask to let the LLM can be shown on web. Using LLM as your personal assistant.

æ­¡è¿ä¾†åˆ° llama Botï¼Œé€™æ˜¯ä¸€å€‹æ™ºèƒ½å°è©±æ©Ÿå™¨äººç¶²ç«™ï¼Œæ¡ç”¨Llama3, flask, langchainå’ŒTailwind CSSæ¡†æ¶ï¼Œç¢ºä¿ä½ åœ¨ä»»ä½•è¨­å‚™ä¸Šéƒ½èƒ½ç²å¾—æœ€ä½³çš„RWDä½¿ç”¨é«”é©—ã€‚

    langchain_community
    langchain_core
    flask 

### Main page
At the main page you can see the beautiful logo I made, because I use Llama3 as LLM, so I put a llama in the middle of logo.ğŸ˜

![](https://github.com/weitsung50110/ollama_flask_langchain_web/blob/main/github_imgs/0.png)

### Navigation bar
You can connect to all my resources, you can find me on linkdin, medium and github.

![](https://github.com/weitsung50110/ollama_flask_langchain_web/blob/main/github_imgs/2.png)

### Input area

![](https://github.com/weitsung50110/ollama_flask_langchain_web/blob/main/github_imgs/4.png)

### Asking llama3 questions
    Who are you?
    
![](https://github.com/weitsung50110/ollama_flask_langchain_web/blob/main/github_imgs/1.png)
    Do you love me?
    
![](https://github.com/weitsung50110/ollama_flask_langchain_web/blob/main/github_imgs/3.png)

## Docker
#### 1. å…ˆå»æŠŠimage pullä¸‹ä¾†

    docker pull weitsung50110/ollama_flask

weitsung50110/ollama_flaskè£¡é¢æˆ‘å·²ç¶“æŠŠollamaå’Œflaskéƒ½å®‰è£å®Œæˆäº†ï¼Œä¸¦æŠŠllama3å’Œllama2ä¹Ÿå·²ç¶“å®‰è£åœ¨æœ¬åœ°äº†ã€‚

#### 2. Run image in order to generate container
 ~/trans_projectæ›è¼‰åˆ°å®¹å™¨å…§çš„appè³‡æ–™å¤¾ä¸‹é¢ï¼Œ ~/trans_projectæ˜¯åœ¨ubuntuä¸­çš„å¯«æ³•ï¼Œè‹¥ä½ æ˜¯windowsè«‹è‡ªè¡Œæ›´æ”¹ä¸¦å‰µå»ºè³‡æ–™å¤¾ã€‚

        docker run -d \
        -v ollama:/root/.ollama \
        -v ~/trans_project:/app \
        -p 5066:5000 \
        -p 11466:11434 \
        --name ollama_flask \
        weitsung50110/ollama_flask:1.0

#### **Portç«¯å£ -p è¬›è§£æ•™å­¸ å¾ˆé‡è¦**
åœ¨ Docker çš„ç«¯å£æ˜ å°„èªæ³• `-p <host_port>:<container_port>` ä¸­ï¼š <br />
- `<host_port>` æ˜¯ä¸»æ©Ÿä¸Šçš„ç«¯å£è™Ÿã€‚ <br />
- `<container_port>` æ˜¯å®¹å™¨å…§éƒ¨çš„ç«¯å£è™Ÿã€‚
 
å› ç‚ºollamaçš„docker imageé è¨­portç«¯å£æ˜¯11434ï¼Œ<br />
æ‰€ä»¥æˆ‘å°‡ä¸»æ©Ÿçš„ 11466 ç«¯å£æ˜ å°„åˆ°ollamaå®¹å™¨çš„ 11434 ç«¯å£ã€‚

å› ç‚ºflaskçš„docker imageé è¨­portç«¯å£æ˜¯5000ï¼Œ<br />
æ‰€ä»¥æˆ‘å°‡ä¸»æ©Ÿçš„ 5066 ç«¯å£æ˜ å°„åˆ°flaskå®¹å™¨çš„ 5000 ç«¯å£ã€‚

`5066` æ˜¯ä¸»æ©Ÿä¸Šçš„ç«¯å£è™Ÿã€‚é€™æ˜¯ä½ ç”¨ä¾†å¾å¤–éƒ¨ï¼ˆä¾‹å¦‚ä½ çš„ç€è¦½å™¨æˆ–å…¶ä»–ç¶²çµ¡å·¥å…·ï¼‰è¨ªå•æ‡‰ç”¨ç¨‹åºçš„ç«¯å£ã€‚<br />
`5000` æ˜¯å®¹å™¨å…§éƒ¨æ‡‰ç”¨ç¨‹åºé‹è¡Œçš„ç«¯å£ã€‚é€™æ˜¯æ‡‰ç”¨ç¨‹åºåœ¨å®¹å™¨å…§éƒ¨ç¶å®šå’Œç›£è½çš„ç«¯å£ã€‚

é€™æ¨£é…ç½®å¾Œï¼Œç•¶ä½ è¨ªå• 127.0.0.1:5066ï¼ˆå‡è¨­ä½ çš„ Docker ä¸»æ©Ÿæ˜¯æœ¬åœ°ä¸»æ©Ÿï¼‰æ™‚ï¼Œå¯¦éš›ä¸Šæ˜¯é€šéä¸»æ©Ÿçš„ 5066 ç«¯å£é€²å…¥å®¹å™¨çš„ 5000 ç«¯å£ï¼Œå¾è€Œè¨ªå•flaskå®¹å™¨å…§éƒ¨é‹è¡Œçš„æ‡‰ç”¨ç¨‹åºLLMã€‚

ç•¶ä½ è¨ªå• 127.0.0.1:11466ï¼Œå¯ä»¥çœ‹åˆ°`Ollama is running`çš„å­—æ¨£ã€‚

#### 3. é€²å…¥ollama_flaskå®¹å™¨conatinerè£¡é¢

        docker exec -it ollama_flask /bin/bash

- è«‹ä¸€å®šè¦å…ˆè¼¸å…¥ä»¥ä¸‹æŒ‡ä»¤

      ollama run llama3

> **_å¦‚æœä½ æ²’æœ‰å…ˆæŠŠllama3å®‰è£åœ¨containerè£¡é¢çš„è©±ï¼Œflaskæœƒç„¡æ³•æ‰“é–‹_**

#### 4. è‹¥æƒ³è¦ä½¿ç”¨ollamaå’Œflaskçµåˆçš„webç¶²é ç‰ˆnlp llmåŠŸèƒ½ï¼Œè«‹å…ˆé€²å…¥ /app/flask ç•¶ä¸­

    cd /app/flask

#### 5. æœƒåœ¨/flaskè³‡æ–™å¤¾ä¸‹é¢çœ‹åˆ°app.pyï¼Œç›´æ¥è¼¸å…¥ä»¥ä¸‹æŒ‡ä»¤

    python3 app.py

#### ä¸€å®šè¦æŒ‡å®športï¼Œ1å€‹containerå¯ä»¥æŒ‡å®šå¤šå€‹port
å¦‚æœä½ docker runçš„æ™‚å€™æ²’æœ‰è¼¸å…¥-p xxxx:5000 å»æŒ‡å®športï¼Œé€™æ¨£å°±æœƒç„¡æ³•ç”¨127.0.0.1:xxxxä¾†é€£é€²flaskè£¡é¢ï¼Œ
- <h4>æ‰€ä»¥runçš„æ™‚å€™ä¸€å®šè¦æŒ‡å®športã€‚</h4>

éŒ¯èª¤ç¯„ä¾‹ >> 
docker run -d 
-v ollama:/root/.ollama 
-v ~/trans_project:/app 
~~-p 5066:5000~~ 
-p 11466:11434 
--name ollama_flask 
weitsung50110/ollama_flask:1.0

#### 6. è·‘æˆåŠŸå°±å¯ä»¥çœ‹åˆ°printä»¥ä¸‹ç•«é¢
printå‡ºä¾†çš„å­—ä¸²ç‚ºRunning on `127.0.0.1:5000`æˆ–`172.17.0.2:5000`ï¼Œä½†**ä¸»è¦ä»¥ä½ docker runæŒ‡å®šçš„portç‚ºæº–**ï¼Œ<br />
è‹¥ä½ çš„portæŒ‡å®š-p 5066:5000ï¼Œå°±è¦è¼¸å…¥ `127.0.0.1:5066`æ‰èƒ½é€£é€²å»å”·~

    root@4be643ba6a94:/app/flask# python3 app2.py
     * Serving Flask app 'app2'
     * Debug mode: on
    INFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.
     * Running on all addresses (0.0.0.0)
     * Running on http://127.0.0.1:5000
     * Running on http://172.17.0.2:5000
    INFO:werkzeug:Press CTRL+C to quit
    INFO:werkzeug: * Restarting with watchdog (inotify)
    WARNING:werkzeug: * Debugger is active!
    INFO:werkzeug: * Debugger PIN: 285-152-236

![](https://github.com/weitsung50110/ollama_flask_langchain_web/blob/main/github_imgs/5.png)
